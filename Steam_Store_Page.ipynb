{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgKc5Tpg3nh9BEB4vXW77I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tbbye/Steam-Data-Collection/blob/main/Steam_Store_Page.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oM40WAdEEzzP",
        "outputId": "ad3ff9cd-e545-4cc4-84b2-4298e3ecb590"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index file steam_index.txt reset.\n",
            "Created/Wiped steam_app_data.csv and wrote headers.\n",
            "Starting at index 0:\n",
            "\n",
            "Exported lines 0-2 to steam_app_data.csv. Batch 0 time: 0:00:05 (avg: 0:00:05, remaining: 0:00:00)\n",
            "\n",
            "Processing batches complete. 3 apps written\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import requests\n",
        "import csv\n",
        "import time\n",
        "import numpy as np\n",
        "import statistics\n",
        "import datetime as dt\n",
        "\n",
        "# customizations - ensure tables show all columns\n",
        "pd.set_option(\"display.max_columns\", 100)\n",
        "\n",
        "def get_request(url, parameters=None):\n",
        "    try:\n",
        "        response = requests.get(url=url, params=parameters)\n",
        "    except requests.exceptions.SSLError as s:\n",
        "        print('SSL Error:', s)\n",
        "\n",
        "        for i in range(5, 0, -1):\n",
        "            print('\\rWaiting... ({})'.format(i), end='')\n",
        "            time.sleep(1)\n",
        "        print('\\rRetrying.' + ' '*10)\n",
        "\n",
        "        # recursively try again\n",
        "        return get_request(url, parameters)\n",
        "\n",
        "    if response:\n",
        "        return response.json()\n",
        "    else:\n",
        "        # response is none usually means too many requests. Wait and try again\n",
        "        print('No response, waiting 10 seconds...')\n",
        "        time.sleep(10)\n",
        "        print('Retrying.')\n",
        "        return get_request(url, parameters)\n",
        "\n",
        "# Manual input of app IDs\n",
        "manual_app_ids = [\"730\", \"570\", \"440\"]  # Add more app IDs as needed\n",
        "\n",
        "# Convert app IDs to DataFrame\n",
        "manual_app_list = pd.DataFrame({'appid': manual_app_ids, 'name': ''})\n",
        "\n",
        "def get_app_data(start, stop, parser, pause):\n",
        "    app_data = []\n",
        "\n",
        "    # iterate through each row of app_list, confined by start and stop\n",
        "    for index, row in manual_app_list[start:stop].iterrows():\n",
        "        print('Current index: {}'.format(index), end='\\r')\n",
        "\n",
        "        appid = row['appid']\n",
        "        name = row['name']\n",
        "\n",
        "        # retrive app data for a row, handled by supplied parser, and append to list\n",
        "        data = parser(appid, name)\n",
        "        app_data.append(data)\n",
        "\n",
        "        time.sleep(pause)  # prevent overloading api with requests\n",
        "\n",
        "    return app_data\n",
        "\n",
        "def process_batches(parser, app_list, download_path, data_filename, index_filename,\n",
        "                    columns, begin=0, end=-1, batchsize=100, pause=1):\n",
        "    print('Starting at index {}:\\n'.format(begin))\n",
        "\n",
        "    if end == -1:\n",
        "        end = len(app_list) + 1\n",
        "\n",
        "    batches = np.arange(begin, end, batchsize)\n",
        "    batches = np.append(batches, end)\n",
        "\n",
        "    apps_written = 0\n",
        "    batch_times = []\n",
        "\n",
        "    for i in range(len(batches) - 1):\n",
        "        start_time = time.time()\n",
        "\n",
        "        start = batches[i]\n",
        "        stop = batches[i + 1]\n",
        "\n",
        "        app_data = get_app_data(start, stop, parser, pause)\n",
        "\n",
        "        rel_path = os.path.join(download_path, data_filename)\n",
        "\n",
        "        with open(rel_path, 'a', newline='', encoding='utf-8') as f:\n",
        "            writer = csv.DictWriter(f, fieldnames=columns, extrasaction='ignore')\n",
        "\n",
        "            for j in range(3, 0, -1):\n",
        "                print(\"\\rAbout to write data, don't stop script! ({})\".format(j), end='')\n",
        "                time.sleep(0.5)\n",
        "\n",
        "            writer.writerows(app_data)\n",
        "            print('\\rExported lines {}-{} to {}.'.format(start, stop - 1, data_filename), end=' ')\n",
        "\n",
        "        apps_written += len(app_data)\n",
        "\n",
        "        idx_path = os.path.join(download_path, index_filename)\n",
        "\n",
        "        with open(idx_path, 'w') as f:\n",
        "            index = stop\n",
        "            print(index, file=f)\n",
        "\n",
        "        end_time = time.time()\n",
        "        time_taken = end_time - start_time\n",
        "\n",
        "        batch_times.append(time_taken)\n",
        "        mean_time = statistics.mean(batch_times)\n",
        "\n",
        "        est_remaining = (len(batches) - i - 2) * mean_time\n",
        "\n",
        "        remaining_td = dt.timedelta(seconds=round(est_remaining))\n",
        "        time_td = dt.timedelta(seconds=round(time_taken))\n",
        "        mean_td = dt.timedelta(seconds=round(mean_time))\n",
        "\n",
        "        print('Batch {} time: {} (avg: {}, remaining: {})'.format(i, time_td, mean_td, remaining_td))\n",
        "\n",
        "    print('\\nProcessing batches complete. {} apps written'.format(apps_written))\n",
        "\n",
        "# --- Start of added helper functions ---\n",
        "\n",
        "def reset_index(download_path, index_filename):\n",
        "    \"\"\"Resets the index file by deleting it.\"\"\"\n",
        "    idx_path = os.path.join(download_path, index_filename)\n",
        "    if os.path.exists(idx_path):\n",
        "        os.remove(idx_path)\n",
        "        print(f'Index file {index_filename} reset.')\n",
        "    else:\n",
        "        print(f'Index file {index_filename} not found, nothing to reset.')\n",
        "\n",
        "def get_index(download_path, index_filename):\n",
        "    \"\"\"Retrieves the last processed index from the index file.\"\"\"\n",
        "    idx_path = os.path.join(download_path, index_filename)\n",
        "    if not os.path.exists(idx_path):\n",
        "        return 0\n",
        "    with open(idx_path, 'r') as f:\n",
        "        try:\n",
        "            index = int(f.read().strip())\n",
        "        except ValueError:\n",
        "            index = 0  # If file is empty or corrupted, start from 0\n",
        "    return index\n",
        "\n",
        "def prepare_data_file(download_path, data_filename, index, columns):\n",
        "    \"\"\"Prepares the data file (creates or wipes) and writes headers if index is 0.\"\"\"\n",
        "    os.makedirs(download_path, exist_ok=True)\n",
        "    rel_path = os.path.join(download_path, data_filename)\n",
        "    mode = 'w' if index == 0 else 'a'\n",
        "    with open(rel_path, mode, newline='', encoding='utf-8') as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=columns, extrasaction='ignore')\n",
        "        if index == 0:\n",
        "            writer.writeheader()\n",
        "            print(f'Created/Wiped {data_filename} and wrote headers.')\n",
        "        else:\n",
        "            print(f'Appending to {data_filename}.')\n",
        "\n",
        "def parse_steam_request(appid, name):\n",
        "    \"\"\"Parses data for a single appid from the Steam API.\"\"\"\n",
        "    url = f\"http://store.steampowered.com/api/appdetails?appids={appid}\"\n",
        "    response_json = get_request(url)\n",
        "\n",
        "    if response_json is None:\n",
        "        print(f\"No response for appid {appid}. Skipping.\")\n",
        "        return {'steam_appid': appid, 'name': name} # Return at least ID and name\n",
        "\n",
        "    try:\n",
        "        # The API returns a dictionary where the key is the appid\n",
        "        app_data = response_json[str(appid)]['data']\n",
        "        app_data['steam_appid'] = appid # Ensure steam_appid is always present\n",
        "        app_data['name'] = name # Ensure name is always present\n",
        "        return app_data\n",
        "    except KeyError:\n",
        "        print(f\"Could not parse data for appid {appid}. Response: {response_json}. Skipping.\")\n",
        "        return {'steam_appid': appid, 'name': name} # Return at least ID and name if parsing fails\n",
        "\n",
        "# --- End of added helper functions ---\n",
        "\n",
        "# Set file parameters\n",
        "download_path = '../data/download'\n",
        "steam_app_data = 'steam_app_data.csv'\n",
        "steam_index = 'steam_index.txt'\n",
        "\n",
        "steam_columns = [\n",
        "    'type', 'name', 'steam_appid', 'required_age', 'is_free', 'controller_support',\n",
        "    'dlc', 'detailed_description', 'about_the_game', 'short_description', 'fullgame',\n",
        "    'supported_languages', 'header_image', 'website', 'pc_requirements', 'mac_requirements',\n",
        "    'linux_requirements', 'legal_notice', 'drm_notice', 'ext_user_account_notice',\n",
        "    'developers', 'publishers', 'demos', 'price_overview', 'packages', 'package_groups',\n",
        "    'platforms', 'metacritic', 'reviews', 'categories', 'genres', 'screenshots',\n",
        "    'movies', 'recommendations', 'achievements', 'release_date', 'support_info',\n",
        "    'background', 'content_descriptors',\n",
        "]\n",
        "\n",
        "# Overwrites last index for demonstration (would usually store highest index so can continue across sessions)\n",
        "reset_index(download_path, steam_index)\n",
        "\n",
        "# Retrieve last index downloaded from file\n",
        "index = get_index(download_path, steam_index)\n",
        "\n",
        "# Wipe or create data file and write headers if index is 0\n",
        "prepare_data_file(download_path, steam_app_data, index, steam_columns)\n",
        "\n",
        "# Set end and chunksize for demonstration - remove to run through entire app list\n",
        "process_batches(\n",
        "    parser=parse_steam_request,\n",
        "    app_list=manual_app_list,\n",
        "    download_path=download_path,\n",
        "    data_filename=steam_app_data,\n",
        "    index_filename=steam_index,\n",
        "    columns=steam_columns,\n",
        "    begin=index,\n",
        "    end=len(manual_app_list),\n",
        "    batchsize=5\n",
        ")\n"
      ]
    }
  ]
}